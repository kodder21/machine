import numpy as np

# define the environment
env = np.array([[-1, -1, -1, -1, 0, -1],
                [-1, -1, -1, 0, -1, 100],
                [-1, -1, -1, 0, -1, -1],
                [-1, 0, 0, -1, 0, -1],
                [0, -1, -1, 0, -1, 100],
                [-1, 0, -1, -1, 0, 100]])

# define the Q matrix
Q = np.zeros((6, 6))

# define hyperparameters
alpha = 0.8
gamma = 0.95
num_episodes = 1000

# Q-learning algorithm
for episode in range(num_episodes):
    # initialize the starting state
    state = np.random.randint(0, 6)
    
    # keep looping until we reach the goal state
    while state != 5:
        # choose an action using the Q matrix and some randomness
        action = np.random.choice(np.where(Q[state] == np.max(Q[state]))[0])
        
        # take the chosen action and observe the next state and reward
        next_state = action
        reward = env[state, next_state]
        
        # update the Q matrix using the Q-learning update rule
        Q[state, action] = (1 - alpha) * Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state]))
        
        # set the next state as the current state
        state = next_state
        
    # print the Q matrix after every 100 episodes
    if (episode + 1) % 100 == 0:
        print("Q matrix after {} episodes:".format(episode + 1))
        print(Q)
        print()

# print the final Q matrix
print("Final Q matrix:")
print(Q)